{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8e33503-0cda-423c-a20e-d8e7f0aa08de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyodbc\n",
    "from datetime import datetime as dt, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45dea67-6591-42a9-b051-2627f995356c",
   "metadata": {},
   "source": [
    "### Feature Store Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6d4e227-cc82-433a-8af8-5d7f76a80430",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsert_sum_multi_key(df1, df2, key_columns, sum_column):\n",
    "    \"\"\"\n",
    "    Performs an upsert operation by summing values in the specified column for matching rows,\n",
    "    using two key columns to determine the matching rows.\n",
    "\n",
    "    Parameters:\n",
    "    - df1 (pd.DataFrame): The first DataFrame (original data).\n",
    "    - df2 (pd.DataFrame): The second DataFrame (data to be upserted).\n",
    "    - key_columns (list): A list of two column names on which to match rows.\n",
    "    - sum_column (str): The numeric column name on which to perform the sum operation.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: A new DataFrame with upserted rows.\n",
    "    \"\"\"\n",
    "    # Merge df1 and df2 on the specified key columns, using an outer join to keep all rows\n",
    "    merged_df = pd.merge(df1, df2, on=key_columns, how=\"outer\", suffixes=('_left', '_right'))\n",
    "    \n",
    "    # Fill NaN with 0 in the sum columns\n",
    "    merged_df[sum_column + '_left'].fillna(0, inplace=True)\n",
    "    merged_df[sum_column + '_right'].fillna(0, inplace=True)\n",
    "    \n",
    "    # Sum the columns for overlapping rows\n",
    "    merged_df[sum_column] = merged_df[sum_column + '_left'] + merged_df[sum_column + '_right']\n",
    "    \n",
    "    # Drop the intermediate columns used for summing\n",
    "    merged_df.drop(columns=[sum_column + '_left', sum_column + '_right'], inplace=True)\n",
    "    \n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbeda15c-d4d9-41d3-82cf-bd2f644f68b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureStore:\n",
    "    \"\"\"\n",
    "    The class is responsible for grabbing data from the SQL Server and transform them to \n",
    "    a Customer-Event-Market granularity, indicating the number of selections on each combination.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 run_datetime = dt.now()\n",
    "                ):\n",
    "\n",
    "        self.run_datetime = run_datetime\n",
    "        self.groupby_cols = ['CustomerKey', 'EventKey', 'EventDeadline', 'MarketKey', \n",
    "                             'MarketName', 'EventName', 'LeagueName', 'SportName','ZoneName'\n",
    "                            ]\n",
    "        self.selections_col = 'BetSelectionKey'\n",
    "        self.total_selections_col = \"total_selections\"\n",
    "        self.run_datetime_col = \"run_datetime\"\n",
    "        self.event_time_colname = \"EventDeadline\"\n",
    "        self.output_df_path = './data_lake/feature_store.csv'\n",
    "        self.connection = pyodbc.connect('Driver={SQL Server};'\n",
    "                                         'Server=BMA-SQL14-SB09\\SB09;'\n",
    "                                         'Database=SBDW;'\n",
    "                                         'Trusted_Connection=yes;'\n",
    "                                         )\n",
    "        self.feature_store_df = None\n",
    "        \n",
    "        self.database_query = \"\"\"\n",
    "                              SELECT * FROM [SBDW].[BR].[BetSelectionData] WITH(NOLOCK)\n",
    "                              \"\"\"\n",
    "        \n",
    "    def extract_from_database(self):\n",
    "        print(\"Extracting Data From SQL Server Database\")\n",
    "        self.feature_store_df = pd.read_sql(self.database_query, self.connection)\n",
    "        print(\"Finished Extracting Data From SQL Server Database\")\n",
    "\n",
    "    def perform_groupby_operations(self):\n",
    "        print(\"Perform groupby operations on dataset: Bring data at the desired granularity level\")\n",
    "        self.feature_store_df_grouped = self.feature_store_df.groupby(self.groupby_cols)[self.selections_col].nunique().reset_index()\n",
    "        print(\"Finished performing groupby operations on dataset\")\n",
    "              \n",
    "    def final_preprocessing(self):\n",
    "         print(\"Executing Final Preprocessing: Renaming Columns and creating new columns\")\n",
    "         self.feature_store_df_grouped.rename(columns={self.selections_col:self.total_selections_col}, inplace=True)\n",
    "         self.feature_store_df_grouped[self.run_datetime_col] = self.run_datetime\n",
    "         print(\"Finished Final Preprocessing Step\")\n",
    "\n",
    "    def write_to_lake(self):\n",
    "        print(\"Started Writing Data to Lake\")\n",
    "        self.feature_store_df_grouped.to_csv(self.output_df_path, index = False, mode = \"w+\")\n",
    "        print(\"Finished Writing Data to Lake\")\n",
    "        \n",
    "    def execute_pipeline(self):\n",
    "        self.extract_from_database()\n",
    "        self.perform_groupby_operations()\n",
    "        self.final_preprocessing()\n",
    "        self.write_to_lake()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9add9948-b9c4-4892-9865-e21d42bca6c7",
   "metadata": {},
   "source": [
    "### Profiles Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7bbb52f3-8dbc-473b-8b7c-1db0a934d592",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_favorites(dataframe,\n",
    "                   major_grouping_col,\n",
    "                   favorite_grouping_col,\n",
    "                   ranking_col,\n",
    "                   ranking_method = \"first\"\n",
    "                  ):\n",
    "\n",
    "    result = dataframe.groupby([major_grouping_col, favorite_grouping_col])[ranking_col].sum().reset_index()\n",
    "\n",
    "    result['rank'] = (result\n",
    "                      .groupby(major_grouping_col)[ranking_col]\n",
    "                      .rank(method=ranking_method, ascending=False)\n",
    "                      .astype(int)\n",
    "                      )\n",
    "    \n",
    "    result = result[result['rank']==1]\n",
    "\n",
    "    return result.loc[:, [major_grouping_col, favorite_grouping_col]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "188e1631-1eca-4974-8c32-c9bdb6133e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomerProfile:\n",
    "\n",
    "    def __init__(self,\n",
    "                ):\n",
    "        \n",
    "        self.feature_store_path = \"./data_lake/feature_store.csv\"\n",
    "        self.major_grouping_col = \"CustomerKey\"\n",
    "        self.ranking_col = \"total_selections\"\n",
    "        self.sport_grouping_col = \"SportName\"\n",
    "        self.league_grouping_col = \"LeagueName\"\n",
    "        self.zone_grouping_col = \"ZoneName\"\n",
    "        self.ranking_method = \"first\"\n",
    "        self.event_time_colname = \"EventDeadline\"\n",
    "        self.customer_profile_dataframe = None\n",
    "        # create the profile based on events up to a month back from now.\n",
    "        self.upper_limit_for_datetime_of_events = dt.now()\n",
    "        self.lower_limit_for_datetime_of_events = dt.now()-timedelta(days=7)\n",
    "        self.output_df_path = './data_lake/customer_profile.csv'\n",
    "        self.connection = pyodbc.connect('Driver={SQL Server};'\n",
    "                                         'Server=BMA-SQL14-SB09\\SB09;'\n",
    "                                         'Database=SBDW;'\n",
    "                                         'Trusted_Connection=yes;'\n",
    "                                         )\n",
    "        self.database_query_for_actives = \"\"\"\n",
    "                                          select distinct CustomerKey\n",
    "                                          from DW.Bet AS b\n",
    "                                          WHERE b.CouponArrivedDate >= CAST(DATEADD(d, -1, GetDate()) as date)\n",
    "                                          \"\"\"\n",
    "        \n",
    "    def _find_favorite_sport(self, dataframe):\n",
    "        favorite_sport_df = find_favorites(dataframe = dataframe,\n",
    "                                           major_grouping_col = self.major_grouping_col,\n",
    "                                           favorite_grouping_col = self.sport_grouping_col,\n",
    "                                           ranking_col = self.ranking_col,\n",
    "                                           ranking_method = self.ranking_method\n",
    "                                           )\n",
    "\n",
    "        return favorite_sport_df\n",
    "\n",
    "    def _find_favorite_league(self, dataframe):\n",
    "        favorite_league_df = find_favorites(dataframe = dataframe,\n",
    "                                            major_grouping_col = self.major_grouping_col,\n",
    "                                            favorite_grouping_col = self.league_grouping_col,\n",
    "                                            ranking_col = self.ranking_col,\n",
    "                                            ranking_method = self.ranking_method\n",
    "                                            )\n",
    "\n",
    "        return favorite_league_df\n",
    "\n",
    "    def _find_favorite_zone(self, dataframe):\n",
    "        favorite_zone_df = find_favorites(dataframe = dataframe,\n",
    "                                          major_grouping_col = self.major_grouping_col,\n",
    "                                          favorite_grouping_col = self.zone_grouping_col,\n",
    "                                          ranking_col = self.ranking_col,\n",
    "                                          ranking_method = self.ranking_method\n",
    "                                          )\n",
    "\n",
    "        return favorite_zone_df\n",
    "        \n",
    "    def get_data_from_lake(self):\n",
    "        feature_store_df = pd.read_csv(self.feature_store_path)\n",
    "        return feature_store_df\n",
    "\n",
    "    def get_actives(self):\n",
    "        df_actives = pd.read_sql(self.database_query_for_actives, self.connection)\n",
    "        return df_actives\n",
    "\n",
    "    def join_actives_and_betting_activity_data(self, feature_store_df, df_actives):\n",
    "        feature_store_df_actives = feature_store_df.merge(df_actives, on = self.major_grouping_col, how = \"inner\")\n",
    "        return feature_store_df_actives\n",
    "    \n",
    "    def get_past_info_and_preprocess(self, feature_store_df_actives):\n",
    "        feature_store_df_actives[self.event_time_colname] = pd.to_datetime(feature_store_df_actives[self.event_time_colname])\n",
    "        df_processed = feature_store_df_actives[(feature_store_df_actives[self.event_time_colname]<=self.upper_limit_for_datetime_of_events)\n",
    "                                                &(feature_store_df_actives[self.event_time_colname]>=self.lower_limit_for_datetime_of_events)\n",
    "                                                ]\n",
    "\n",
    "        return df_processed\n",
    "        \n",
    "    def create_customer_profile_dataframe(self, df_processed):\n",
    "        favorite_sport_df = self._find_favorite_sport(df_processed)\n",
    "        favorite_league_df = self._find_favorite_league(df_processed)\n",
    "        favorite_zone_df = self._find_favorite_zone(df_processed)\n",
    "        customer_profile_dataframe = (favorite_sport_df\n",
    "                                       .merge(favorite_league_df, on = self.major_grouping_col, how = \"inner\")\n",
    "                                       .merge(favorite_zone_df, on = self.major_grouping_col, how = \"inner\")\n",
    "                                      )\n",
    "\n",
    "        return customer_profile_dataframe\n",
    "        \n",
    "    def write_to_lake(self):\n",
    "        self.customer_profile_dataframe.to_csv(self.output_df_path, index = False)\n",
    "\n",
    "\n",
    "    def execute_pipeline(self):\n",
    "        feature_store_df = self.get_data_from_lake()\n",
    "        df_actives = self.get_actives()\n",
    "        feature_store_df_actives = self.join_actives_and_betting_activity_data(feature_store_df = feature_store_df, df_actives=df_actives)\n",
    "        df_processed = self.get_past_info_and_preprocess(feature_store_df_actives)\n",
    "        self.customer_profile_dataframe = self.create_customer_profile_dataframe(df_processed)\n",
    "        self.write_to_lake()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b43eaa77-e6c5-44b8-9427-431406b5eaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EventProfile:\n",
    "    \n",
    "    def __init__(self,\n",
    "                ):\n",
    "\n",
    "        self.events_df = None\n",
    "        self.connection = pyodbc.connect('Driver={SQL Server};'\n",
    "                                         'Server=BMA-SQL14-SB09\\SB09;'\n",
    "                                         'Database=SBDW;'\n",
    "                                         'Trusted_Connection=yes;'\n",
    "                                         )\n",
    "\n",
    "        # query for tomorrow's events, so that only those are recommended.\n",
    "        self.database_query = \"\"\"\n",
    "                              select events.EventKey,  \n",
    "                              events.EventName,\n",
    "                              events.EventDeadline,\n",
    "                              sport.CategoryName as SportName, \n",
    "                              league.SubcategoryName as LeagueName,\n",
    "                              zone.SubCategoryGroupName as ZoneName\n",
    "                              from DW.Event as events\n",
    "                              inner join DW.SubCategory as league WITH(NOLOCK)\n",
    "                              on events.SubCategoryKey = league.SubCategoryKey\n",
    "                              inner join DW.Category as sport WITH(NOLOCK)\n",
    "                              on sport.CategoryKey = league.CategoryKey\n",
    "                              inner join DW.SubCategoryGroup as zone WITH(NOLOCK)\n",
    "                              on league.SubCategoryGroupKey = zone.SubCategoryGroupKey\n",
    "                              where EventDeadline >= CAST(DATEADD(d, 1, GetDate()) as date)\n",
    "                              and EventDeadline <= CAST(DATEADD(d, 2, GetDate()) as date)\n",
    "                              and sport.CategoryName in ('Football', 'Basketball', 'Tennis')\n",
    "                              and league.SubcategoryName not like 'Test%'\n",
    "                              and zone.SubCategoryGroupName not in ('FIFA', 'NBA2K')\n",
    "                              and EventTypeKey = 2;\n",
    "                              \"\"\"\n",
    "        \n",
    "        self.output_df_path = './data_lake/event_profile.csv'\n",
    "\n",
    "    def extract_from_database(self):\n",
    "        print(\"Extracting Data From SQL Server Database\")\n",
    "        self.events_df = pd.read_sql(self.database_query, self.connection)\n",
    "        print(\"Finished Data From SQL Server Database\")\n",
    "\n",
    "    \n",
    "    def preprocessing(self):\n",
    "        pass\n",
    "\n",
    "    \n",
    "    def write_to_lake(self):\n",
    "        print(\"Started Writing Data to Lake\")\n",
    "        self.events_df.to_csv(self.output_df_path, index = False, mode='w+')\n",
    "        print(\"Finished Writing Data to Lake\")\n",
    "\n",
    "\n",
    "    def execute_pipeline(self):\n",
    "        self.extract_from_database()\n",
    "        self.preprocessing()\n",
    "        self.write_to_lake()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "673754f4-e266-4d46-af34-d3b2fff4944d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecommendationEngine:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.nlp = spacy.load(\"en_core_web_md\")\n",
    "        self.customer_profiles_path = \"./data_lake/customer_profile.csv\"\n",
    "        self.event_profiles_path = \"./data_lake/event_profile.csv\"\n",
    "        self.league_name_colname = 'LeagueName'\n",
    "        self.zone_name_colnmame = 'ZoneName'\n",
    "        self.sport_name_colname = \"SportName\"\n",
    "        self.customer_key_colname = 'CustomerKey'\n",
    "        self.event_key_colname = 'EventKey'\n",
    "        self.similarity_colname = \"similarity\"\n",
    "        self.rank_colname = \"rank\"\n",
    "        self.run_datetime_colname = \"run_datetime\"\n",
    "        self.grouping_profile_cols = [self.sport_name_colname, self.league_name_colname, self.zone_name_colnmame]\n",
    "        self.similarity_league_weight = 0.4\n",
    "        self.similarity_zone_weight = 0.6\n",
    "        self.top_n_recommendations_to_provide = 3\n",
    "        self.run_datetime = dt.now()\n",
    "        self.df_final_similarities = None\n",
    "        self.output_df_path = './data_lake/customer_event_recommendations.csv'\n",
    "    \n",
    "    def get_event_and_customer_profiles(self):\n",
    "        self.customer_profiles = pd.read_csv(self.customer_profiles_path)\n",
    "        self.event_profiles = pd.read_csv(self.event_profiles_path)\n",
    "        \n",
    "    def create_event_and_customer_profile_groups(self):\n",
    "        self.customer_profile_elements  = (self.customer_profiles\n",
    "                                          .groupby(self.grouping_profile_cols)\n",
    "                                          .agg(customers_list = (self.customer_key_colname , 'unique'))\n",
    "                                          .reset_index()\n",
    "                                          )\n",
    "        \n",
    "        self.event_profile_elements = (self.event_profiles\n",
    "                                      .groupby(self.grouping_profile_cols)\n",
    "                                      .agg(events_list = (self.event_key_colname , 'unique'))\n",
    "                                      .reset_index()\n",
    "                                      )\n",
    "        \n",
    "    def calculate_similarities(self):\n",
    "        list_of_dataframes = []\n",
    "        for index_customer, row_customer in self.customer_profile_elements.iterrows():\n",
    "            print(\"Running loop: \", index_customer, \"/\", len(self.customer_profile_elements), \"customer profiles completed\")\n",
    "            df_similarities = pd.DataFrame()\n",
    "            events_to_check = self.event_profile_elements[self.event_profile_elements[self.sport_name_colname]==row_customer[self.sport_name_colname]]\n",
    "            for index_event, row_event in events_to_check.iterrows():\n",
    "                customer_league_text = row_customer[self.league_name_colname]\n",
    "                customer_zone_text = row_customer[self.zone_name_colnmame]\n",
    "                event_league_text = row_event[self.league_name_colname]\n",
    "                event_zone_text = row_event[self.zone_name_colnmame]\n",
    "            \n",
    "                similarity_league = self.similarity_league_weight*self.nlp(customer_league_text).similarity(self.nlp(event_league_text))\n",
    "                similarity_zone = self.similarity_zone_weight*self.nlp(customer_zone_text).similarity(self.nlp(event_zone_text))\n",
    "                total_similarity = similarity_league + similarity_zone\n",
    "                \n",
    "                row_dict = {self.customer_key_colname: row_customer['customers_list'],\n",
    "                            self.event_key_colname: row_event['events_list'],\n",
    "                            self.similarity_colname: total_similarity\n",
    "                           }\n",
    "                \n",
    "                df_similarities = df_similarities._append(row_dict, ignore_index=True)\n",
    "            list_of_dataframes.append(df_similarities)\n",
    "        \n",
    "        self.df_final_similarities = pd.concat(list_of_dataframes)\n",
    "        \n",
    "    def final_preprocessing(self):\n",
    "        self.ranking_dataframe = self.df_final_similarities.explode(self.customer_key_colname).explode(self.event_key_colname)\n",
    "        self.ranking_dataframe[self.customer_key_colname] = self.ranking_dataframe[self.customer_key_colname].astype(\"int\")\n",
    "        self.ranking_dataframe[self.rank_colname] = (self.ranking_dataframe\n",
    "                                                      .groupby(self.customer_key_colname)[self.similarity_colname]\n",
    "                                                      .rank(method=\"first\", ascending=False)\n",
    "                                                      .astype(int)\n",
    "                                                      )\n",
    "        \n",
    "        self.ranking_dataframe = (self.ranking_dataframe[self.ranking_dataframe[self.rank_colname]<=self.top_n_recommendations_to_provide]\n",
    "                                 .sort_values([self.customer_key_colname, self.rank_colname], ascending=[True, True])\n",
    "                                 .reset_index(drop=True)\n",
    "                                )\n",
    "        \n",
    "        self.ranking_dataframe[self.run_datetime_colname] = self.run_datetime\n",
    "        \n",
    "    def write_to_lake(self):\n",
    "        self.ranking_dataframe.to_csv(self.output_df_path, index = False, mode='w+')\n",
    "        \n",
    "    def execute_pipeline(self):\n",
    "        self.get_event_and_customer_profiles()\n",
    "        self.create_event_and_customer_profile_groups()\n",
    "        self.calculate_similarities()\n",
    "        self.final_preprocessing()\n",
    "        self.write_to_lake()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
